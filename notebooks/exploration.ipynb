{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83f17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shamika/miniconda3/envs/ml311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d1c7b",
   "metadata": {},
   "source": [
    "## LLM quick test\n",
    "\n",
    "1. Set `MODEL_PATH` to your local checkpoint directory.\n",
    "2. Run the next cells to generate answers and (optionally) score a small GSM8K slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Point this to your local model folder\n",
    "MODEL_PATH = \"\"  # e.g. \"/path/to/Qwen2.5-1.5B-Instruct\"\n",
    "if not MODEL_PATH:\n",
    "    raise ValueError(\"Set MODEL_PATH to your local model directory\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    bf16_ok = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
    "    dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Allow imports whether you run from repo root or from notebooks/\n",
    "if Path(\"grpo_homework.py\").exists():\n",
    "    repo_root = Path(\".\")\n",
    "elif Path(\"../grpo_homework.py\").exists():\n",
    "    repo_root = Path(\"..\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Couldn't find grpo_homework.py; run this notebook from the repo root.\")\n",
    "sys.path.append(str(repo_root.resolve()))\n",
    "\n",
    "from grpo_homework import GSM8KDataset, extract_answer_from_completion, compute_reward\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded:\", MODEL_PATH)\n",
    "print(\"Device:\", device, \"dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(question: str) -> str:\n",
    "    return f\"Question: {question}\\nAnswer: Let's solve this step by step.\\n\"\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_completion(\n",
    "    question: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    do_sample: bool = False,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.95,\n",
    ") -> str:\n",
    "    prompt = format_prompt(question)\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    output_ids = model.generate(\n",
    "        **encoded,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )[0]\n",
    "    prompt_len = encoded[\"input_ids\"].shape[1]\n",
    "    return tokenizer.decode(output_ids[prompt_len:], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"If a store sells 3 apples for $2, how much do 12 apples cost?\"\n",
    "completion = generate_completion(question, do_sample=False, max_new_tokens=128)\n",
    "print(format_prompt(question) + completion)\n",
    "print(\"\\nExtracted answer:\", extract_answer_from_completion(completion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small GSM8K sanity check (keep this small; generation can be slow)\n",
    "ds = load_dataset(str(repo_root / \"gsm8k\"), \"main\", split=\"test[:5]\")\n",
    "\n",
    "completions = []\n",
    "gt_answers = []\n",
    "for i in range(len(ds)):\n",
    "    q = ds[i][\"question\"]\n",
    "    gt = GSM8KDataset.extract_answer(ds[i][\"answer\"])\n",
    "    comp = generate_completion(q, do_sample=False, max_new_tokens=128)\n",
    "    completions.append(comp)\n",
    "    gt_answers.append(gt)\n",
    "    pred = extract_answer_from_completion(comp)\n",
    "    print(f\"\\n[{i}] pred={pred} gt={gt}\\n{comp[:500]}\")\n",
    "\n",
    "rewards = compute_reward(completions, gt_answers)\n",
    "print(\"\\nAccuracy:\", rewards.mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
